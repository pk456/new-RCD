nohup: ignoring input
/home/liujiaqi/miniconda3/envs/dpl/lib/python3.9/site-packages/dgl/heterograph.py:92: DGLWarning: Recommend creating graphs by `dgl.graph(data)` instead of `dgl.DGLGraph(data)`.
  dgl_warning(
Net(
  (knowledge_emb): Embedding(835, 64)
  (exercise_emb): Embedding(835, 64)
  (student_emb): Embedding(10000, 64)
  (FusionLayer1): Fusion(
    (directed_gat): GraphLayer(
      (fc): Linear(in_features=64, out_features=64, bias=False)
      (attn_fc): Linear(in_features=128, out_features=1, bias=False)
    )
    (undirected_gat): GraphLayer(
      (fc): Linear(in_features=64, out_features=64, bias=False)
      (attn_fc): Linear(in_features=128, out_features=1, bias=False)
    )
    (e_to_k): GraphLayer(
      (fc): Linear(in_features=64, out_features=64, bias=False)
      (attn_fc): Linear(in_features=128, out_features=1, bias=False)
    )
    (k_to_e): GraphLayer(
      (fc): Linear(in_features=64, out_features=64, bias=False)
      (attn_fc): Linear(in_features=128, out_features=1, bias=False)
    )
    (e_to_u): GraphLayer(
      (fc): Linear(in_features=64, out_features=64, bias=False)
      (attn_fc): Linear(in_features=128, out_features=1, bias=False)
    )
    (u_to_e): GraphLayer(
      (fc): Linear(in_features=64, out_features=64, bias=False)
      (attn_fc): Linear(in_features=128, out_features=1, bias=False)
    )
    (k_attn_fc1): Linear(in_features=128, out_features=1, bias=True)
    (k_attn_fc2): Linear(in_features=128, out_features=1, bias=True)
    (k_attn_fc3): Linear(in_features=128, out_features=1, bias=True)
    (e_attn_fc1): Linear(in_features=128, out_features=1, bias=True)
    (e_attn_fc2): Linear(in_features=128, out_features=1, bias=True)
  )
  (FusionLayer2): Fusion(
    (directed_gat): GraphLayer(
      (fc): Linear(in_features=64, out_features=64, bias=False)
      (attn_fc): Linear(in_features=128, out_features=1, bias=False)
    )
    (undirected_gat): GraphLayer(
      (fc): Linear(in_features=64, out_features=64, bias=False)
      (attn_fc): Linear(in_features=128, out_features=1, bias=False)
    )
    (e_to_k): GraphLayer(
      (fc): Linear(in_features=64, out_features=64, bias=False)
      (attn_fc): Linear(in_features=128, out_features=1, bias=False)
    )
    (k_to_e): GraphLayer(
      (fc): Linear(in_features=64, out_features=64, bias=False)
      (attn_fc): Linear(in_features=128, out_features=1, bias=False)
    )
    (e_to_u): GraphLayer(
      (fc): Linear(in_features=64, out_features=64, bias=False)
      (attn_fc): Linear(in_features=128, out_features=1, bias=False)
    )
    (u_to_e): GraphLayer(
      (fc): Linear(in_features=64, out_features=64, bias=False)
      (attn_fc): Linear(in_features=128, out_features=1, bias=False)
    )
    (k_attn_fc1): Linear(in_features=128, out_features=1, bias=True)
    (k_attn_fc2): Linear(in_features=128, out_features=1, bias=True)
    (k_attn_fc3): Linear(in_features=128, out_features=1, bias=True)
    (e_attn_fc1): Linear(in_features=128, out_features=1, bias=True)
    (e_attn_fc2): Linear(in_features=128, out_features=1, bias=True)
  )
  (prednet_full1): Linear(in_features=128, out_features=64, bias=False)
  (prednet_full2): Linear(in_features=128, out_features=64, bias=False)
  (prednet_full3): Linear(in_features=64, out_features=1, bias=True)
)
training model...
train--1/20 [9/1090] loss: 0.6083795189857483
train--1/20 [19/1090] loss: 0.6726457953453064
train--1/20 [29/1090] loss: 0.670920068025589
train--1/20 [39/1090] loss: 0.670836192369461
train--1/20 [49/1090] loss: 0.6729426503181457
train--1/20 [59/1090] loss: 0.667334258556366
train--1/20 [69/1090] loss: 0.667387193441391
train--1/20 [79/1090] loss: 0.6642074584960938
train--1/20 [89/1090] loss: 0.6570094645023346
train--1/20 [99/1090] loss: 0.6547155499458313
train--1/20 [109/1090] loss: 0.6528165161609649
train--1/20 [119/1090] loss: 0.646812254190445
train--1/20 [129/1090] loss: 0.6373451232910157
train--1/20 [139/1090] loss: 0.629178524017334
train--1/20 [149/1090] loss: 0.6257123589515686
train--1/20 [159/1090] loss: 0.6176912903785705
train--1/20 [169/1090] loss: 0.6193530440330506
train--1/20 [179/1090] loss: 0.601651257276535
train--1/20 [189/1090] loss: 0.6082338035106659
train--1/20 [199/1090] loss: 0.5841406166553498
train--1/20 [209/1090] loss: 0.582607626914978
train--1/20 [219/1090] loss: 0.5699423432350159
train--1/20 [229/1090] loss: 0.563003408908844
train--1/20 [239/1090] loss: 0.5708788990974426
train--1/20 [249/1090] loss: 0.550475013256073
train--1/20 [259/1090] loss: 0.558773934841156
train--1/20 [269/1090] loss: 0.5516780257225037
train--1/20 [279/1090] loss: 0.5616302251815796
train--1/20 [289/1090] loss: 0.5409032166004181
train--1/20 [299/1090] loss: 0.5468239903450012
train--1/20 [309/1090] loss: 0.5437165856361389
train--1/20 [319/1090] loss: 0.5395424485206604
train--1/20 [329/1090] loss: 0.5398251593112946
train--1/20 [339/1090] loss: 0.5307433366775512
train--1/20 [349/1090] loss: 0.5246783316135406
train--1/20 [359/1090] loss: 0.520208540558815
train--1/20 [369/1090] loss: 0.526826822757721
train--1/20 [379/1090] loss: 0.5222143441438675
train--1/20 [389/1090] loss: 0.5130954205989837
train--1/20 [399/1090] loss: 0.5215429157018662
train--1/20 [409/1090] loss: 0.5212298810482026
train--1/20 [419/1090] loss: 0.5274903744459152
train--1/20 [429/1090] loss: 0.5124663054943085
train--1/20 [439/1090] loss: 0.5292505770921707
train--1/20 [449/1090] loss: 0.5154831498861313
train--1/20 [459/1090] loss: 0.5019938975572587
train--1/20 [469/1090] loss: 0.5221916705369949
train--1/20 [479/1090] loss: 0.5214279890060425
train--1/20 [489/1090] loss: 0.5180946081876755
train--1/20 [499/1090] loss: 0.5286696404218674
train--1/20 [509/1090] loss: 0.5199539929628372
train--1/20 [519/1090] loss: 0.5024494469165802
train--1/20 [529/1090] loss: 0.5068168640136719
train--1/20 [539/1090] loss: 0.5250805914402008
train--1/20 [549/1090] loss: 0.524945643544197
train--1/20 [559/1090] loss: 0.5259134143590927
train--1/20 [569/1090] loss: 0.5162249356508255
train--1/20 [579/1090] loss: 0.5181669950485229
train--1/20 [589/1090] loss: 0.511492469906807
train--1/20 [599/1090] loss: 0.5015177339315414
train--1/20 [609/1090] loss: 0.5090459793806076
train--1/20 [619/1090] loss: 0.5091929316520691
train--1/20 [629/1090] loss: 0.5163899570703506
train--1/20 [639/1090] loss: 0.5320674240589142
train--1/20 [649/1090] loss: 0.52681605219841
train--1/20 [659/1090] loss: 0.510995066165924
train--1/20 [669/1090] loss: 0.5025872230529785
train--1/20 [679/1090] loss: 0.5003403514623642
train--1/20 [689/1090] loss: 0.510412585735321
train--1/20 [699/1090] loss: 0.5052985906600952
train--1/20 [709/1090] loss: 0.5286058217287064
train--1/20 [719/1090] loss: 0.4978299796581268
train--1/20 [729/1090] loss: 0.5193907648324967
train--1/20 [739/1090] loss: 0.5169637441635132
train--1/20 [749/1090] loss: 0.5073423206806182
train--1/20 [759/1090] loss: 0.5093084037303924
train--1/20 [769/1090] loss: 0.4971407115459442
train--1/20 [779/1090] loss: 0.5360284566879272
train--1/20 [789/1090] loss: 0.5012374639511108
train--1/20 [799/1090] loss: 0.5072166502475739
train--1/20 [809/1090] loss: 0.5019704431295395
train--1/20 [819/1090] loss: 0.527730667591095
train--1/20 [829/1090] loss: 0.5146272540092468
train--1/20 [839/1090] loss: 0.5215851485729217
train--1/20 [849/1090] loss: 0.5028380990028382
train--1/20 [859/1090] loss: 0.5244638055562973
train--1/20 [869/1090] loss: 0.48911004364490507
train--1/20 [879/1090] loss: 0.5052378058433533
train--1/20 [889/1090] loss: 0.498073211312294
train--1/20 [899/1090] loss: 0.4981499433517456
train--1/20 [909/1090] loss: 0.5114924818277359
train--1/20 [919/1090] loss: 0.519209298491478
train--1/20 [929/1090] loss: 0.5125697493553162
train--1/20 [939/1090] loss: 0.5093643665313721
train--1/20 [949/1090] loss: 0.4966665506362915
train--1/20 [959/1090] loss: 0.4900285392999649
train--1/20 [969/1090] loss: 0.5053253769874573
train--1/20 [979/1090] loss: 0.4882208287715912
train--1/20 [989/1090] loss: 0.5044242560863494
train--1/20 [999/1090] loss: 0.506581699848175
train--1/20 [1009/1090] loss: 0.5045137614011764
train--1/20 [1019/1090] loss: 0.49352465867996215
train--1/20 [1029/1090] loss: 0.4955955445766449
train--1/20 [1039/1090] loss: 0.4829623639583588
train--1/20 [1049/1090] loss: 0.5119077682495117
train--1/20 [1059/1090] loss: 0.506311959028244
train--1/20 [1069/1090] loss: 0.4828528553247452
train--1/20 [1079/1090] loss: 0.4903836607933044
train--1/20 [1089/1090] loss: 0.4941824346780777
predicting model...
val--1/20 [10/1090] acc: 0.7609375
val--1/20 [20/1090] acc: 0.761328125
val--1/20 [30/1090] acc: 0.759375
val--1/20 [40/1090] acc: 0.761328125
val--1/20 [50/1090] acc: 0.760078125
val--1/20 [60/1090] acc: 0.7591145833333334
val--1/20 [70/1090] acc: 0.7579799107142857
val--1/20 [80/1090] acc: 0.758203125
val--1/20 [90/1090] acc: 0.7596788194444445
val--1/20 [100/1090] acc: 0.7596484375
val--1/20 [110/1090] acc: 0.7595170454545455
val--1/20 [120/1090] acc: 0.7581705729166667
val--1/20 [130/1090] acc: 0.7599158653846154
val--1/20 [140/1090] acc: 0.7599051339285714
val--1/20 [150/1090] acc: 0.7602604166666667
val--1/20 [160/1090] acc: 0.760791015625
val--1/20 [170/1090] acc: 0.7602481617647059
val--1/20 [180/1090] acc: 0.7601128472222223
val--1/20 [190/1090] acc: 0.760485197368421
val--1/20 [200/1090] acc: 0.7605078125
val--1/20 [210/1090] acc: 0.7609561011904762
val--1/20 [220/1090] acc: 0.7603870738636364
val--1/20 [230/1090] acc: 0.7606827445652173
val--1/20 [240/1090] acc: 0.7603190104166667
val--1/20 [250/1090] acc: 0.76003125
val--1/20 [260/1090] acc: 0.7601262019230769
val--1/20 [270/1090] acc: 0.7604456018518518
val--1/20 [280/1090] acc: 0.76064453125
val--1/20 [290/1090] acc: 0.7606411637931034
val--1/20 [300/1090] acc: 0.760703125
val--1/20 [310/1090] acc: 0.7606350806451613
val--1/20 [320/1090] acc: 0.7608642578125
val--1/20 [330/1090] acc: 0.7607599431818182
val--1/20 [340/1090] acc: 0.7609145220588235
val--1/20 [350/1090] acc: 0.7608816964285714
val--1/20 [360/1090] acc: 0.7606228298611111
val--1/20 [370/1090] acc: 0.7608002533783784
val--1/20 [380/1090] acc: 0.7607113486842105
val--1/20 [390/1090] acc: 0.7604567307692308
val--1/20 [400/1090] acc: 0.760556640625
val--1/20 [410/1090] acc: 0.7608708079268293
val--1/20 [420/1090] acc: 0.761142113095238
val--1/20 [430/1090] acc: 0.7611555232558139
val--1/20 [440/1090] acc: 0.7611772017045455
val--1/20 [450/1090] acc: 0.7609201388888889
val--1/20 [460/1090] acc: 0.7609375
val--1/20 [470/1090] acc: 0.7608045212765957
val--1/20 [480/1090] acc: 0.7609456380208334
val--1/20 [490/1090] acc: 0.760985331632653
val--1/20 [500/1090] acc: 0.761234375
val--1/20 [510/1090] acc: 0.7610753676470589
val--1/20 [520/1090] acc: 0.7612229567307692
val--1/20 [530/1090] acc: 0.7611217570754717
val--1/20 [540/1090] acc: 0.7612774884259259
val--1/20 [550/1090] acc: 0.7612073863636364
val--1/20 [560/1090] acc: 0.7613141741071429
val--1/20 [570/1090] acc: 0.7611910635964912
val--1/20 [580/1090] acc: 0.7611732219827586
val--1/20 [590/1090] acc: 0.761083156779661
val--1/20 [600/1090] acc: 0.7607877604166666
val--1/20 [610/1090] acc: 0.760828637295082
val--1/20 [620/1090] acc: 0.7608114919354839
val--1/20 [630/1090] acc: 0.760999503968254
val--1/20 [640/1090] acc: 0.761248779296875
val--1/20 [650/1090] acc: 0.7612560096153846
val--1/20 [660/1090] acc: 0.7612097537878788
val--1/20 [670/1090] acc: 0.7609783115671642
val--1/20 [680/1090] acc: 0.7609604779411765
val--1/20 [690/1090] acc: 0.7610846920289855
val--1/20 [700/1090] acc: 0.7609877232142858
val--1/20 [710/1090] acc: 0.7609925176056338
val--1/20 [720/1090] acc: 0.7608832465277777
val--1/20 [730/1090] acc: 0.7608893407534246
val--1/20 [740/1090] acc: 0.7608372043918918
val--1/20 [750/1090] acc: 0.7608802083333334
val--1/20 [760/1090] acc: 0.7609118009868421
val--1/20 [770/1090] acc: 0.7608664772727273
val--1/20 [780/1090] acc: 0.7609425080128205
val--1/20 [790/1090] acc: 0.7610413370253165
val--1/20 [800/1090] acc: 0.7612646484375
val--1/20 [810/1090] acc: 0.7612943672839506
val--1/20 [820/1090] acc: 0.7613805259146341
val--1/20 [830/1090] acc: 0.7614787274096385
val--1/20 [840/1090] acc: 0.76142578125
val--1/20 [850/1090] acc: 0.7615027573529412
val--1/20 [860/1090] acc: 0.7616869549418605
val--1/20 [870/1090] acc: 0.7616873204022988
val--1/20 [880/1090] acc: 0.7617231889204545
val--1/20 [890/1090] acc: 0.7619601474719101
val--1/20 [900/1090] acc: 0.7620182291666666
val--1/20 [910/1090] acc: 0.7621308379120879
val--1/20 [920/1090] acc: 0.7620754076086956
val--1/20 [930/1090] acc: 0.7621219758064516
val--1/20 [940/1090] acc: 0.7621758643617021
val--1/20 [950/1090] acc: 0.7621792763157895
val--1/20 [960/1090] acc: 0.7620686848958333
val--1/20 [970/1090] acc: 0.762109375
val--1/20 [980/1090] acc: 0.7621293048469387
val--1/20 [990/1090] acc: 0.7620620265151515
val--1/20 [1000/1090] acc: 0.7619296875
val--1/20 [1010/1090] acc: 0.7617574257425742
val--1/20 [1020/1090] acc: 0.7618795955882353
val--1/20 [1030/1090] acc: 0.761961468446602
val--1/20 [1040/1090] acc: 0.7620192307692307
val--1/20 [1050/1090] acc: 0.7621056547619047
val--1/20 [1060/1090] acc: 0.7619951356132075
val--1/20 [1070/1090] acc: 0.7619889018691589
val--1/20 [1080/1090] acc: 0.7619466145833333
val--1/20 [1090/1090] acc: 0.7620018635321101
epoch= 1, accuracy= 0.762002, rmse= 0.403745, auc= 0.813505
train--2/20 [9/1090] loss: 0.4364533364772797
train--2/20 [19/1090] loss: 0.48426189422607424
train--2/20 [29/1090] loss: 0.5062215685844421
train--2/20 [39/1090] loss: 0.4954045921564102
train--2/20 [49/1090] loss: 0.49080135226249694
train--2/20 [59/1090] loss: 0.4862286329269409
train--2/20 [69/1090] loss: 0.48960068821907043
train--2/20 [79/1090] loss: 0.4924867421388626
train--2/20 [89/1090] loss: 0.49902138113975525
train--2/20 [99/1090] loss: 0.4995720416307449
train--2/20 [109/1090] loss: 0.49092579782009127
train--2/20 [119/1090] loss: 0.48952566385269164
train--2/20 [129/1090] loss: 0.4732275724411011
train--2/20 [139/1090] loss: 0.5105311512947083
train--2/20 [149/1090] loss: 0.482093009352684
train--2/20 [159/1090] loss: 0.498937851190567
train--2/20 [169/1090] loss: 0.49808808267116544
train--2/20 [179/1090] loss: 0.49653408825397494
train--2/20 [189/1090] loss: 0.5000977247953415
train--2/20 [199/1090] loss: 0.4895095229148865
train--2/20 [209/1090] loss: 0.4900441110134125
train--2/20 [219/1090] loss: 0.5049125671386718
train--2/20 [229/1090] loss: 0.476065394282341
train--2/20 [239/1090] loss: 0.49542294144630433
train--2/20 [249/1090] loss: 0.48803112506866453
train--2/20 [259/1090] loss: 0.491207018494606
train--2/20 [269/1090] loss: 0.4767196446657181
train--2/20 [279/1090] loss: 0.477824205160141
train--2/20 [289/1090] loss: 0.46632889211177825
train--2/20 [299/1090] loss: 0.4854243963956833
train--2/20 [309/1090] loss: 0.4748529762029648
train--2/20 [319/1090] loss: 0.4962343841791153
train--2/20 [329/1090] loss: 0.48601357638835907
train--2/20 [339/1090] loss: 0.5026491016149521
train--2/20 [349/1090] loss: 0.4843125015497208
train--2/20 [359/1090] loss: 0.4773193091154099
train--2/20 [369/1090] loss: 0.4795828968286514
train--2/20 [379/1090] loss: 0.4794207692146301
train--2/20 [389/1090] loss: 0.46850536167621615
train--2/20 [399/1090] loss: 0.48664447069168093
train--2/20 [409/1090] loss: 0.4813124448060989
train--2/20 [419/1090] loss: 0.47733561396598817
train--2/20 [429/1090] loss: 0.4881531894207001
train--2/20 [439/1090] loss: 0.47817625999450686
train--2/20 [449/1090] loss: 0.469292214512825
train--2/20 [459/1090] loss: 0.4939074069261551
train--2/20 [469/1090] loss: 0.4898906111717224
train--2/20 [479/1090] loss: 0.48103207051754
train--2/20 [489/1090] loss: 0.48280655443668363
train--2/20 [499/1090] loss: 0.47607471644878385
train--2/20 [509/1090] loss: 0.5018203616142273
train--2/20 [519/1090] loss: 0.4752475738525391
train--2/20 [529/1090] loss: 0.47633797824382784
train--2/20 [539/1090] loss: 0.4702810287475586
train--2/20 [549/1090] loss: 0.48469275534152984
train--2/20 [559/1090] loss: 0.46410414576530457
train--2/20 [569/1090] loss: 0.4843718230724335
train--2/20 [579/1090] loss: 0.4859381943941116
train--2/20 [589/1090] loss: 0.4951546460390091
train--2/20 [599/1090] loss: 0.48684760332107546
train--2/20 [609/1090] loss: 0.478304722905159
train--2/20 [619/1090] loss: 0.4914649218320847
train--2/20 [629/1090] loss: 0.4846381634473801
train--2/20 [639/1090] loss: 0.4840610891580582
train--2/20 [649/1090] loss: 0.4977247714996338
train--2/20 [659/1090] loss: 0.4698257058858871
train--2/20 [669/1090] loss: 0.48993834257125857
train--2/20 [679/1090] loss: 0.483309218287468
train--2/20 [689/1090] loss: 0.4909893870353699
train--2/20 [699/1090] loss: 0.4621303558349609
train--2/20 [709/1090] loss: 0.5042523324489594
train--2/20 [719/1090] loss: 0.46552381217479705
train--2/20 [729/1090] loss: 0.46443712115287783
train--2/20 [739/1090] loss: 0.4770172029733658
train--2/20 [749/1090] loss: 0.4934524565935135
train--2/20 [759/1090] loss: 0.4663660377264023
train--2/20 [769/1090] loss: 0.47405576705932617
train--2/20 [779/1090] loss: 0.48850962817668914
train--2/20 [789/1090] loss: 0.4727871775627136
train--2/20 [799/1090] loss: 0.4561254054307938
train--2/20 [809/1090] loss: 0.4575958251953125
train--2/20 [819/1090] loss: 0.47895156443119047
train--2/20 [829/1090] loss: 0.47994834184646606
train--2/20 [839/1090] loss: 0.47617898881435394
train--2/20 [849/1090] loss: 0.4966235101222992
train--2/20 [859/1090] loss: 0.47579880654811857
train--2/20 [869/1090] loss: 0.47315452992916107
train--2/20 [879/1090] loss: 0.4919850260019302
train--2/20 [889/1090] loss: 0.45769180059432985
train--2/20 [899/1090] loss: 0.4828203558921814
train--2/20 [909/1090] loss: 0.46084407567977903
train--2/20 [919/1090] loss: 0.46721145510673523
train--2/20 [929/1090] loss: 0.48263599574565885
train--2/20 [939/1090] loss: 0.4716925650835037
train--2/20 [949/1090] loss: 0.48811420500278474
train--2/20 [959/1090] loss: 0.47421238422393797
train--2/20 [969/1090] loss: 0.4807760685682297
train--2/20 [979/1090] loss: 0.4844260483980179
train--2/20 [989/1090] loss: 0.49172326922416687
train--2/20 [999/1090] loss: 0.4881071537733078
train--2/20 [1009/1090] loss: 0.47030931115150454
train--2/20 [1019/1090] loss: 0.4745018780231476
train--2/20 [1029/1090] loss: 0.48918484151363373
train--2/20 [1039/1090] loss: 0.45826704502105714
train--2/20 [1049/1090] loss: 0.47041488587856295
train--2/20 [1059/1090] loss: 0.47820442318916323
train--2/20 [1069/1090] loss: 0.4692266434431076
train--2/20 [1079/1090] loss: 0.4877451181411743
train--2/20 [1089/1090] loss: 0.47178274393081665
predicting model...
val--2/20 [10/1090] acc: 0.777734375
val--2/20 [20/1090] acc: 0.778125
val--2/20 [30/1090] acc: 0.7776041666666667
val--2/20 [40/1090] acc: 0.77841796875
val--2/20 [50/1090] acc: 0.7778125
val--2/20 [60/1090] acc: 0.7770182291666666
val--2/20 [70/1090] acc: 0.777734375
val--2/20 [80/1090] acc: 0.77724609375
val--2/20 [90/1090] acc: 0.7779513888888889
val--2/20 [100/1090] acc: 0.7793359375
val--2/20 [110/1090] acc: 0.7791548295454546
val--2/20 [120/1090] acc: 0.7783528645833333
val--2/20 [130/1090] acc: 0.7798978365384616
val--2/20 [140/1090] acc: 0.7801339285714286
val--2/20 [150/1090] acc: 0.7802604166666667
val--2/20 [160/1090] acc: 0.780126953125
val--2/20 [170/1090] acc: 0.7796875
val--2/20 [180/1090] acc: 0.7794704861111111
val--2/20 [190/1090] acc: 0.7800986842105263
val--2/20 [200/1090] acc: 0.7803515625
val--2/20 [210/1090] acc: 0.7806175595238095
val--2/20 [220/1090] acc: 0.7797762784090909
val--2/20 [230/1090] acc: 0.7803838315217392
val--2/20 [240/1090] acc: 0.7800618489583333
val--2/20 [250/1090] acc: 0.78003125
val--2/20 [260/1090] acc: 0.7801682692307692
val--2/20 [270/1090] acc: 0.7805121527777777
val--2/20 [280/1090] acc: 0.7808175223214285
val--2/20 [290/1090] acc: 0.7809536637931035
val--2/20 [300/1090] acc: 0.7809635416666667
val--2/20 [310/1090] acc: 0.7806577620967742
val--2/20 [320/1090] acc: 0.78089599609375
val--2/20 [330/1090] acc: 0.7808830492424242
val--2/20 [340/1090] acc: 0.7809857536764706
val--2/20 [350/1090] acc: 0.78125
val--2/20 [360/1090] acc: 0.7811848958333333
val--2/20 [370/1090] acc: 0.7812922297297298
val--2/20 [380/1090] acc: 0.7812191611842105
val--2/20 [390/1090] acc: 0.7810396634615384
val--2/20 [400/1090] acc: 0.78109375
val--2/20 [410/1090] acc: 0.7813071646341463
val--2/20 [420/1090] acc: 0.7813151041666667
val--2/20 [430/1090] acc: 0.7812590843023256
val--2/20 [440/1090] acc: 0.7813299005681819
val--2/20 [450/1090] acc: 0.7811805555555555
val--2/20 [460/1090] acc: 0.781148097826087
val--2/20 [470/1090] acc: 0.780967420212766
val--2/20 [480/1090] acc: 0.7811360677083333
val--2/20 [490/1090] acc: 0.781234056122449
val--2/20 [500/1090] acc: 0.7815234375
val--2/20 [510/1090] acc: 0.7813878676470588
val--2/20 [520/1090] acc: 0.7815504807692307
val--2/20 [530/1090] acc: 0.7815153301886792
val--2/20 [540/1090] acc: 0.7816623263888889
val--2/20 [550/1090] acc: 0.7815411931818181
val--2/20 [560/1090] acc: 0.7816336495535714
val--2/20 [570/1090] acc: 0.7815104166666667
val--2/20 [580/1090] acc: 0.7815867456896551
val--2/20 [590/1090] acc: 0.7816009004237288
val--2/20 [600/1090] acc: 0.7813997395833333
val--2/20 [610/1090] acc: 0.7813396516393443
val--2/20 [620/1090] acc: 0.781281502016129
val--2/20 [630/1090] acc: 0.7815662202380952
val--2/20 [640/1090] acc: 0.7819091796875
val--2/20 [650/1090] acc: 0.7818870192307692
val--2/20 [660/1090] acc: 0.7818063446969697
val--2/20 [670/1090] acc: 0.7814832089552238
val--2/20 [680/1090] acc: 0.7813534007352941
val--2/20 [690/1090] acc: 0.7815613677536232
val--2/20 [700/1090] acc: 0.7815513392857143
val--2/20 [710/1090] acc: 0.781481073943662
val--2/20 [720/1090] acc: 0.7813585069444444
val--2/20 [730/1090] acc: 0.7812928082191781
val--2/20 [740/1090] acc: 0.7812288851351351
val--2/20 [750/1090] acc: 0.7812708333333334
val--2/20 [760/1090] acc: 0.7813013980263158
val--2/20 [770/1090] acc: 0.7812905844155844
val--2/20 [780/1090] acc: 0.7813251201923077
val--2/20 [790/1090] acc: 0.7814181170886076
val--2/20 [800/1090] acc: 0.781611328125
val--2/20 [810/1090] acc: 0.7816358024691358
val--2/20 [820/1090] acc: 0.7817549542682927
val--2/20 [830/1090] acc: 0.7818194653614458
val--2/20 [840/1090] acc: 0.7816638764880952
val--2/20 [850/1090] acc: 0.7816314338235294
val--2/20 [860/1090] acc: 0.7818132267441861
val--2/20 [870/1090] acc: 0.7817394037356322
val--2/20 [880/1090] acc: 0.7817294034090909
val--2/20 [890/1090] acc: 0.7819961376404494
val--2/20 [900/1090] acc: 0.7819618055555555
val--2/20 [910/1090] acc: 0.781979739010989
val--2/20 [920/1090] acc: 0.7819251019021739
val--2/20 [930/1090] acc: 0.7820480510752689
val--2/20 [940/1090] acc: 0.7820935837765958
val--2/20 [950/1090] acc: 0.7820559210526316
val--2/20 [960/1090] acc: 0.7819498697916667
val--2/20 [970/1090] acc: 0.7819144652061856
val--2/20 [980/1090] acc: 0.7819674744897959
val--2/20 [990/1090] acc: 0.7819049873737374
val--2/20 [1000/1090] acc: 0.78173828125
val--2/20 [1010/1090] acc: 0.781512995049505
val--2/20 [1020/1090] acc: 0.7816444546568627
val--2/20 [1030/1090] acc: 0.7816937196601942
val--2/20 [1040/1090] acc: 0.7817082331730769
val--2/20 [1050/1090] acc: 0.7817373511904762
val--2/20 [1060/1090] acc: 0.781640625
val--2/20 [1070/1090] acc: 0.7816442757009345
val--2/20 [1080/1090] acc: 0.7816225405092593
val--2/20 [1090/1090] acc: 0.781586869266055
epoch= 2, accuracy= 0.781587, rmse= 0.388903, auc= 0.839490
train--3/20 [9/1090] loss: 0.421497768163681
train--3/20 [19/1090] loss: 0.46856012642383577
train--3/20 [29/1090] loss: 0.46679910719394685
train--3/20 [39/1090] loss: 0.4457824766635895
train--3/20 [49/1090] loss: 0.45911900997161864
train--3/20 [59/1090] loss: 0.46567364037036896
train--3/20 [69/1090] loss: 0.4579419821500778
train--3/20 [79/1090] loss: 0.47901995182037355
train--3/20 [89/1090] loss: 0.4612636625766754
train--3/20 [99/1090] loss: 0.4670336306095123
train--3/20 [109/1090] loss: 0.4440274089574814
train--3/20 [119/1090] loss: 0.46510040760040283
train--3/20 [129/1090] loss: 0.47481603026390073
train--3/20 [139/1090] loss: 0.4442425429821014
train--3/20 [149/1090] loss: 0.4558508187532425
train--3/20 [159/1090] loss: 0.4687493830919266
train--3/20 [169/1090] loss: 0.45865886509418485
train--3/20 [179/1090] loss: 0.46108597218990327
train--3/20 [189/1090] loss: 0.46478303968906404
train--3/20 [199/1090] loss: 0.43812794983386993
train--3/20 [209/1090] loss: 0.4632965803146362
train--3/20 [219/1090] loss: 0.4704576343297958
train--3/20 [229/1090] loss: 0.4711511075496674
train--3/20 [239/1090] loss: 0.46940380036830903
train--3/20 [249/1090] loss: 0.4624347001314163
train--3/20 [259/1090] loss: 0.45025585889816283
train--3/20 [269/1090] loss: 0.46257991194725034
train--3/20 [279/1090] loss: 0.4661432683467865
train--3/20 [289/1090] loss: 0.4680948853492737
train--3/20 [299/1090] loss: 0.46826828122138975
train--3/20 [309/1090] loss: 0.4586873948574066
train--3/20 [319/1090] loss: 0.46042410731315614
train--3/20 [329/1090] loss: 0.4678151607513428
train--3/20 [339/1090] loss: 0.4474017918109894
train--3/20 [349/1090] loss: 0.46689389944076537
train--3/20 [359/1090] loss: 0.47630725502967836
train--3/20 [369/1090] loss: 0.44590420424938204
train--3/20 [379/1090] loss: 0.47141246497631073
train--3/20 [389/1090] loss: 0.451609143614769
train--3/20 [399/1090] loss: 0.4888707011938095
train--3/20 [409/1090] loss: 0.4570340722799301
train--3/20 [419/1090] loss: 0.4557875573635101
train--3/20 [429/1090] loss: 0.47808848321437836
train--3/20 [439/1090] loss: 0.460460302233696
train--3/20 [449/1090] loss: 0.4688441753387451
train--3/20 [459/1090] loss: 0.46701025366783144
train--3/20 [469/1090] loss: 0.4741798937320709
train--3/20 [479/1090] loss: 0.45996433198451997
train--3/20 [489/1090] loss: 0.4667005270719528
train--3/20 [499/1090] loss: 0.4538552135229111
train--3/20 [509/1090] loss: 0.45791810750961304
train--3/20 [519/1090] loss: 0.4490832030773163
train--3/20 [529/1090] loss: 0.4557558536529541
train--3/20 [539/1090] loss: 0.46207364797592165
train--3/20 [549/1090] loss: 0.4800493955612183
train--3/20 [559/1090] loss: 0.4619913846254349
train--3/20 [569/1090] loss: 0.45636684000492095
train--3/20 [579/1090] loss: 0.4649583429098129
train--3/20 [589/1090] loss: 0.4865860491991043
train--3/20 [599/1090] loss: 0.4414021968841553
train--3/20 [609/1090] loss: 0.46561970114707946
train--3/20 [619/1090] loss: 0.4722487300634384
train--3/20 [629/1090] loss: 0.45120933949947356
train--3/20 [639/1090] loss: 0.4458814710378647
train--3/20 [649/1090] loss: 0.48529151678085325
train--3/20 [659/1090] loss: 0.4673702836036682
train--3/20 [669/1090] loss: 0.45942688882350924
train--3/20 [679/1090] loss: 0.4494893878698349
train--3/20 [689/1090] loss: 0.4655885845422745
train--3/20 [699/1090] loss: 0.4608534395694733
train--3/20 [709/1090] loss: 0.4511806219816208
train--3/20 [719/1090] loss: 0.4576506167650223
train--3/20 [729/1090] loss: 0.47333219945430755
train--3/20 [739/1090] loss: 0.4542790621519089
train--3/20 [749/1090] loss: 0.46490844786167146
train--3/20 [759/1090] loss: 0.4540001690387726
train--3/20 [769/1090] loss: 0.45657303035259245
train--3/20 [779/1090] loss: 0.4373945683240891
train--3/20 [789/1090] loss: 0.46497536897659303
train--3/20 [799/1090] loss: 0.48542152643203734
train--3/20 [809/1090] loss: 0.45984964072704315
train--3/20 [819/1090] loss: 0.4501099050045013
train--3/20 [829/1090] loss: 0.46777819395065307
train--3/20 [839/1090] loss: 0.4742236852645874
train--3/20 [849/1090] loss: 0.45958860218524933
train--3/20 [859/1090] loss: 0.45802501440048216
train--3/20 [869/1090] loss: 0.46026717126369476
train--3/20 [879/1090] loss: 0.4767245024442673
train--3/20 [889/1090] loss: 0.46637911796569825
train--3/20 [899/1090] loss: 0.4669137537479401
train--3/20 [909/1090] loss: 0.466508162021637
train--3/20 [919/1090] loss: 0.4645931899547577
train--3/20 [929/1090] loss: 0.4786704212427139
train--3/20 [939/1090] loss: 0.4658373087644577
train--3/20 [949/1090] loss: 0.4601709097623825
train--3/20 [959/1090] loss: 0.46123426258563993
train--3/20 [969/1090] loss: 0.47058839201927183
train--3/20 [979/1090] loss: 0.45720421373844145
train--3/20 [989/1090] loss: 0.4563653767108917
train--3/20 [999/1090] loss: 0.44809792637825013
train--3/20 [1009/1090] loss: 0.4765276998281479
train--3/20 [1019/1090] loss: 0.45793538391590116
train--3/20 [1029/1090] loss: 0.46103056967258454
train--3/20 [1039/1090] loss: 0.45611982345581054
train--3/20 [1049/1090] loss: 0.47157648801803587
train--3/20 [1059/1090] loss: 0.45396042466163633
train--3/20 [1069/1090] loss: 0.4565076917409897
train--3/20 [1079/1090] loss: 0.4388448387384415
train--3/20 [1089/1090] loss: 0.458833310008049
predicting model...
val--3/20 [10/1090] acc: 0.7921875
val--3/20 [20/1090] acc: 0.790625
val--3/20 [30/1090] acc: 0.7908854166666667
val--3/20 [40/1090] acc: 0.7904296875
val--3/20 [50/1090] acc: 0.78828125
val--3/20 [60/1090] acc: 0.7872395833333333
val--3/20 [70/1090] acc: 0.7869419642857143
val--3/20 [80/1090] acc: 0.7861328125
val--3/20 [90/1090] acc: 0.786328125
val--3/20 [100/1090] acc: 0.7875
val--3/20 [110/1090] acc: 0.7877130681818182
val--3/20 [120/1090] acc: 0.7879231770833334
val--3/20 [130/1090] acc: 0.7893028846153847
val--3/20 [140/1090] acc: 0.7892857142857143
val--3/20 [150/1090] acc: 0.7894791666666666
val--3/20 [160/1090] acc: 0.7894775390625
val--3/20 [170/1090] acc: 0.7893612132352941
val--3/20 [180/1090] acc: 0.7890407986111111
val--3/20 [190/1090] acc: 0.7898643092105263
val--3/20 [200/1090] acc: 0.79001953125
val--3/20 [210/1090] acc: 0.7904017857142858
val--3/20 [220/1090] acc: 0.7894353693181818
val--3/20 [230/1090] acc: 0.7902173913043479
val--3/20 [240/1090] acc: 0.7897135416666666
val--3/20 [250/1090] acc: 0.78975
val--3/20 [260/1090] acc: 0.789873798076923
val--3/20 [270/1090] acc: 0.7900752314814815
val--3/20 [280/1090] acc: 0.7902901785714286
val--3/20 [290/1090] acc: 0.7902478448275863
val--3/20 [300/1090] acc: 0.7901953125
val--3/20 [310/1090] acc: 0.7897681451612903
val--3/20 [320/1090] acc: 0.79000244140625
val--3/20 [330/1090] acc: 0.7900568181818182
val--3/20 [340/1090] acc: 0.7901654411764706
val--3/20 [350/1090] acc: 0.7904799107142857
val--3/20 [360/1090] acc: 0.7903428819444445
val--3/20 [370/1090] acc: 0.7903927364864864
val--3/20 [380/1090] acc: 0.7902549342105263
val--3/20 [390/1090] acc: 0.7900340544871794
val--3/20 [400/1090] acc: 0.7899609375
val--3/20 [410/1090] acc: 0.7901200457317074
val--3/20 [420/1090] acc: 0.7901971726190476
val--3/20 [430/1090] acc: 0.7900890261627908
val--3/20 [440/1090] acc: 0.7901100852272728
val--3/20 [450/1090] acc: 0.7900086805555555
val--3/20 [460/1090] acc: 0.789937160326087
val--3/20 [470/1090] acc: 0.7897772606382979
val--3/20 [480/1090] acc: 0.7898844401041667
val--3/20 [490/1090] acc: 0.7900031887755102
val--3/20 [500/1090] acc: 0.7902890625
val--3/20 [510/1090] acc: 0.7901654411764706
val--3/20 [520/1090] acc: 0.7903771033653846
val--3/20 [530/1090] acc: 0.7904260023584906
val--3/20 [540/1090] acc: 0.7905381944444444
val--3/20 [550/1090] acc: 0.7903338068181818
val--3/20 [560/1090] acc: 0.7902483258928571
val--3/20 [570/1090] acc: 0.7900699013157895
val--3/20 [580/1090] acc: 0.790106411637931
val--3/20 [590/1090] acc: 0.7900357521186441
val--3/20 [600/1090] acc: 0.7899544270833333
val--3/20 [610/1090] acc: 0.7898245389344263
val--3/20 [620/1090] acc: 0.7898878528225807
val--3/20 [630/1090] acc: 0.7900979662698413
val--3/20 [640/1090] acc: 0.790338134765625
val--3/20 [650/1090] acc: 0.7903966346153846
val--3/20 [660/1090] acc: 0.7903586647727273
val--3/20 [670/1090] acc: 0.7900827891791045
val--3/20 [680/1090] acc: 0.7898954503676471
val--3/20 [690/1090] acc: 0.7900588768115943
val--3/20 [700/1090] acc: 0.7901227678571429
val--3/20 [710/1090] acc: 0.7900088028169014
val--3/20 [720/1090] acc: 0.7898274739583333
val--3/20 [730/1090] acc: 0.7897581335616438
val--3/20 [740/1090] acc: 0.7896537162162162
val--3/20 [750/1090] acc: 0.7897864583333334
val--3/20 [760/1090] acc: 0.78984375
val--3/20 [770/1090] acc: 0.78984375
val--3/20 [780/1090] acc: 0.7898137019230769
val--3/20 [790/1090] acc: 0.7898486946202532
val--3/20 [800/1090] acc: 0.7900927734375
val--3/20 [810/1090] acc: 0.7901234567901234
val--3/20 [820/1090] acc: 0.7902057926829268
val--3/20 [830/1090] acc: 0.7903002635542169
val--3/20 [840/1090] acc: 0.7902622767857143
val--3/20 [850/1090] acc: 0.7902619485294118
val--3/20 [860/1090] acc: 0.7904660247093023
val--3/20 [870/1090] acc: 0.7903421336206896
val--3/20 [880/1090] acc: 0.7903009588068182
val--3/20 [890/1090] acc: 0.790528441011236
val--3/20 [900/1090] acc: 0.7904730902777778
val--3/20 [910/1090] acc: 0.7904790521978022
val--3/20 [920/1090] acc: 0.7904551630434783
val--3/20 [930/1090] acc: 0.7905451948924731
val--3/20 [940/1090] acc: 0.7905875997340426
val--3/20 [950/1090] acc: 0.7904769736842105
val--3/20 [960/1090] acc: 0.7903564453125
val--3/20 [970/1090] acc: 0.7902665914948453
val--3/20 [980/1090] acc: 0.7903180803571429
val--3/20 [990/1090] acc: 0.7902383207070707
val--3/20 [1000/1090] acc: 0.79012890625
val--3/20 [1010/1090] acc: 0.7900177908415842
val--3/20 [1020/1090] acc: 0.7901041666666667
val--3/20 [1030/1090] acc: 0.7901585254854369
val--3/20 [1040/1090] acc: 0.7901742788461539
val--3/20 [1050/1090] acc: 0.7901599702380953
val--3/20 [1060/1090] acc: 0.7900943396226415
val--3/20 [1070/1090] acc: 0.7900773948598131
val--3/20 [1080/1090] acc: 0.7900209780092593
val--3/20 [1090/1090] acc: 0.7898903383027523
epoch= 3, accuracy= 0.789890, rmse= 0.381988, auc= 0.849817
train--4/20 [9/1090] loss: 0.39270569384098053
train--4/20 [19/1090] loss: 0.43906072080135344
train--4/20 [29/1090] loss: 0.46401874125003817
train--4/20 [39/1090] loss: 0.44142200648784635
train--4/20 [49/1090] loss: 0.4370024293661118
train--4/20 [59/1090] loss: 0.44017966091632843
train--4/20 [69/1090] loss: 0.4528988629579544
train--4/20 [79/1090] loss: 0.4443208247423172
train--4/20 [89/1090] loss: 0.4456245243549347
train--4/20 [99/1090] loss: 0.4504342913627625
train--4/20 [109/1090] loss: 0.44569764733314515
train--4/20 [119/1090] loss: 0.46480334401130674
train--4/20 [129/1090] loss: 0.44513643383979795
train--4/20 [139/1090] loss: 0.4664356172084808
train--4/20 [149/1090] loss: 0.46691651046276095
train--4/20 [159/1090] loss: 0.45412522852420806
train--4/20 [169/1090] loss: 0.441934871673584
train--4/20 [179/1090] loss: 0.469358691573143
train--4/20 [189/1090] loss: 0.4710393399000168
train--4/20 [199/1090] loss: 0.45991426408290864
train--4/20 [209/1090] loss: 0.45956208407878874
train--4/20 [219/1090] loss: 0.44528651535511016
train--4/20 [229/1090] loss: 0.4445368528366089
train--4/20 [239/1090] loss: 0.4645869493484497
train--4/20 [249/1090] loss: 0.4584969639778137
train--4/20 [259/1090] loss: 0.44053452014923095
train--4/20 [269/1090] loss: 0.4598018705844879
train--4/20 [279/1090] loss: 0.4467926204204559
train--4/20 [289/1090] loss: 0.4558916449546814
train--4/20 [299/1090] loss: 0.44602503478527067
train--4/20 [309/1090] loss: 0.47142982184886933
train--4/20 [319/1090] loss: 0.44631094932556153
train--4/20 [329/1090] loss: 0.4516010284423828
train--4/20 [339/1090] loss: 0.45621252059936523
train--4/20 [349/1090] loss: 0.45754876732826233
train--4/20 [359/1090] loss: 0.47401273548603057
train--4/20 [369/1090] loss: 0.44206353425979616
train--4/20 [379/1090] loss: 0.4265747249126434
train--4/20 [389/1090] loss: 0.46954638361930845
train--4/20 [399/1090] loss: 0.44247678518295286
train--4/20 [409/1090] loss: 0.4577913761138916
train--4/20 [419/1090] loss: 0.44826788604259493
train--4/20 [429/1090] loss: 0.45180646181106565
train--4/20 [439/1090] loss: 0.43821811079978945
train--4/20 [449/1090] loss: 0.47406097650527956
train--4/20 [459/1090] loss: 0.46155228912830354
train--4/20 [469/1090] loss: 0.45607993602752683
train--4/20 [479/1090] loss: 0.4442558288574219
train--4/20 [489/1090] loss: 0.4584705322980881
train--4/20 [499/1090] loss: 0.4439485251903534
train--4/20 [509/1090] loss: 0.45355749428272246
train--4/20 [519/1090] loss: 0.4630573272705078
train--4/20 [529/1090] loss: 0.4509228318929672
train--4/20 [539/1090] loss: 0.45710547268390656
train--4/20 [549/1090] loss: 0.46591454446315766
train--4/20 [559/1090] loss: 0.447378209233284
train--4/20 [569/1090] loss: 0.4570487320423126
train--4/20 [579/1090] loss: 0.4395958960056305
train--4/20 [589/1090] loss: 0.44962312579154967
train--4/20 [599/1090] loss: 0.4726993560791016
train--4/20 [609/1090] loss: 0.4456862360239029
train--4/20 [619/1090] loss: 0.4582516372203827
train--4/20 [629/1090] loss: 0.4590178787708282
train--4/20 [639/1090] loss: 0.4578930616378784
train--4/20 [649/1090] loss: 0.45917268097400665
train--4/20 [659/1090] loss: 0.4603849083185196
train--4/20 [669/1090] loss: 0.45298430919647215
train--4/20 [679/1090] loss: 0.4615582138299942
train--4/20 [689/1090] loss: 0.4559252500534058
train--4/20 [699/1090] loss: 0.4808121085166931
train--4/20 [709/1090] loss: 0.4699253857135773
train--4/20 [719/1090] loss: 0.46956162452697753
train--4/20 [729/1090] loss: 0.44700844287872316
train--4/20 [739/1090] loss: 0.44100695550441743
train--4/20 [749/1090] loss: 0.4503829151391983
train--4/20 [759/1090] loss: 0.4418335407972336
train--4/20 [769/1090] loss: 0.4825154602527618
train--4/20 [779/1090] loss: 0.44884495437145233
train--4/20 [789/1090] loss: 0.4553238540887833
train--4/20 [799/1090] loss: 0.47653238773345946
train--4/20 [809/1090] loss: 0.45267401933670043
train--4/20 [819/1090] loss: 0.44566574692726135
train--4/20 [829/1090] loss: 0.4545902520418167
train--4/20 [839/1090] loss: 0.4689922422170639
train--4/20 [849/1090] loss: 0.45525943338871
train--4/20 [859/1090] loss: 0.47385938465595245
train--4/20 [869/1090] loss: 0.45964818000793456
train--4/20 [879/1090] loss: 0.4580656111240387
train--4/20 [889/1090] loss: 0.4545577704906464
train--4/20 [899/1090] loss: 0.4521786093711853
train--4/20 [909/1090] loss: 0.46691291630268095
train--4/20 [919/1090] loss: 0.45540142357349395
train--4/20 [929/1090] loss: 0.46960105299949645
train--4/20 [939/1090] loss: 0.4576000332832336
train--4/20 [949/1090] loss: 0.46361362636089326
train--4/20 [959/1090] loss: 0.441921466588974
train--4/20 [969/1090] loss: 0.4543693423271179
train--4/20 [979/1090] loss: 0.45081134140491486
train--4/20 [989/1090] loss: 0.450118824839592
train--4/20 [999/1090] loss: 0.46165881752967836
train--4/20 [1009/1090] loss: 0.44914473593235016
train--4/20 [1019/1090] loss: 0.44992893040180204
train--4/20 [1029/1090] loss: 0.45719448626041415
train--4/20 [1039/1090] loss: 0.46075376868247986
train--4/20 [1049/1090] loss: 0.44038166105747223
train--4/20 [1059/1090] loss: 0.43887296617031096
train--4/20 [1069/1090] loss: 0.48825485706329347
train--4/20 [1079/1090] loss: 0.43125344514846803
train--4/20 [1089/1090] loss: 0.44428570568561554
predicting model...
val--4/20 [10/1090] acc: 0.792578125
val--4/20 [20/1090] acc: 0.7908203125
val--4/20 [30/1090] acc: 0.7893229166666667
val--4/20 [40/1090] acc: 0.790625
val--4/20 [50/1090] acc: 0.78953125
val--4/20 [60/1090] acc: 0.7892578125
val--4/20 [70/1090] acc: 0.7901227678571429
val--4/20 [80/1090] acc: 0.789013671875
val--4/20 [90/1090] acc: 0.7893229166666667
val--4/20 [100/1090] acc: 0.7902734375
val--4/20 [110/1090] acc: 0.7901988636363636
val--4/20 [120/1090] acc: 0.7896158854166667
val--4/20 [130/1090] acc: 0.7906550480769231
val--4/20 [140/1090] acc: 0.7906808035714286
val--4/20 [150/1090] acc: 0.7907291666666667
val--4/20 [160/1090] acc: 0.790771484375
val--4/20 [170/1090] acc: 0.7906020220588236
val--4/20 [180/1090] acc: 0.7904947916666667
val--4/20 [190/1090] acc: 0.7910773026315789
val--4/20 [200/1090] acc: 0.79134765625
val--4/20 [210/1090] acc: 0.791592261904762
val--4/20 [220/1090] acc: 0.7905362215909091
val--4/20 [230/1090] acc: 0.7912194293478261
val--4/20 [240/1090] acc: 0.790869140625
val--4/20 [250/1090] acc: 0.790828125
val--4/20 [260/1090] acc: 0.7909104567307692
val--4/20 [270/1090] acc: 0.7911892361111111
val--4/20 [280/1090] acc: 0.7912248883928571
val--4/20 [290/1090] acc: 0.791082974137931
val--4/20 [300/1090] acc: 0.7912760416666667
val--4/20 [310/1090] acc: 0.7909526209677419
val--4/20 [320/1090] acc: 0.79117431640625
val--4/20 [330/1090] acc: 0.791311553030303
val--4/20 [340/1090] acc: 0.7914751838235294
val--4/20 [350/1090] acc: 0.7916741071428571
val--4/20 [360/1090] acc: 0.7915473090277778
val--4/20 [370/1090] acc: 0.7915962837837838
val--4/20 [380/1090] acc: 0.7916015625
val--4/20 [390/1090] acc: 0.7913561698717949
val--4/20 [400/1090] acc: 0.791298828125
val--4/20 [410/1090] acc: 0.7914443597560976
val--4/20 [420/1090] acc: 0.7915178571428572
val--4/20 [430/1090] acc: 0.7913971656976744
val--4/20 [440/1090] acc: 0.7914950284090909
val--4/20 [450/1090] acc: 0.791328125
val--4/20 [460/1090] acc: 0.7912703804347826
val--4/20 [470/1090] acc: 0.791123670212766
val--4/20 [480/1090] acc: 0.7912516276041667
val--4/20 [490/1090] acc: 0.7913105867346939
val--4/20 [500/1090] acc: 0.7914609375
val--4/20 [510/1090] acc: 0.7913679534313726
val--4/20 [520/1090] acc: 0.7915640024038462
val--4/20 [530/1090] acc: 0.7915905070754717
val--4/20 [540/1090] acc: 0.7916956018518518
val--4/20 [550/1090] acc: 0.791484375
val--4/20 [560/1090] acc: 0.7913992745535714
val--4/20 [570/1090] acc: 0.7911389802631579
val--4/20 [580/1090] acc: 0.7911570581896552
val--4/20 [590/1090] acc: 0.7911546610169492
val--4/20 [600/1090] acc: 0.7910026041666667
val--4/20 [610/1090] acc: 0.7908811475409836
val--4/20 [620/1090] acc: 0.7908392137096775
val--4/20 [630/1090] acc: 0.7910714285714285
val--4/20 [640/1090] acc: 0.791314697265625
val--4/20 [650/1090] acc: 0.7913521634615385
val--4/20 [660/1090] acc: 0.7912642045454545
val--4/20 [670/1090] acc: 0.7909223414179104
val--4/20 [680/1090] acc: 0.7907513786764706
val--4/20 [690/1090] acc: 0.7908401268115942
val--4/20 [700/1090] acc: 0.7909095982142857
val--4/20 [710/1090] acc: 0.7907460387323944
val--4/20 [720/1090] acc: 0.7905436197916667
val--4/20 [730/1090] acc: 0.790480522260274
val--4/20 [740/1090] acc: 0.7903663429054054
val--4/20 [750/1090] acc: 0.790515625
val--4/20 [760/1090] acc: 0.7905684621710526
val--4/20 [770/1090] acc: 0.7906148538961039
val--4/20 [780/1090] acc: 0.7905548878205129
val--4/20 [790/1090] acc: 0.7905903876582279
val--4/20 [800/1090] acc: 0.79080078125
val--4/20 [810/1090] acc: 0.7908709490740741
val--4/20 [820/1090] acc: 0.7909394054878048
val--4/20 [830/1090] acc: 0.7910250376506024
val--4/20 [840/1090] acc: 0.7910063244047619
val--4/20 [850/1090] acc: 0.7910707720588235
val--4/20 [860/1090] acc: 0.7912881540697675
val--4/20 [870/1090] acc: 0.7911503232758621
val--4/20 [880/1090] acc: 0.7911532315340909
val--4/20 [890/1090] acc: 0.7913228581460674
val--4/20 [900/1090] acc: 0.7912977430555556
val--4/20 [910/1090] acc: 0.7912774725274725
val--4/20 [920/1090] acc: 0.7913043478260869
val--4/20 [930/1090] acc: 0.7913936491935484
val--4/20 [940/1090] acc: 0.7914478058510638
val--4/20 [950/1090] acc: 0.7914021381578947
val--4/20 [960/1090] acc: 0.7912923177083333
val--4/20 [970/1090] acc: 0.7912210051546392
val--4/20 [980/1090] acc: 0.7912826849489796
val--4/20 [990/1090] acc: 0.7912089646464646
val--4/20 [1000/1090] acc: 0.7911484375
val--4/20 [1010/1090] acc: 0.7909924195544554
val--4/20 [1020/1090] acc: 0.7910577512254902
val--4/20 [1030/1090] acc: 0.7911028519417476
val--4/20 [1040/1090] acc: 0.7910907451923077
val--4/20 [1050/1090] acc: 0.7911793154761905
val--4/20 [1060/1090] acc: 0.7911224941037736
val--4/20 [1070/1090] acc: 0.7911032418224299
val--4/20 [1080/1090] acc: 0.7910083912037037
val--4/20 [1090/1090] acc: 0.7908471903669725
epoch= 4, accuracy= 0.790847, rmse= 0.380650, auc= 0.852050
train--5/20 [9/1090] loss: 0.3910861015319824
train--5/20 [19/1090] loss: 0.45025692582130433
train--5/20 [29/1090] loss: 0.43379202485084534
train--5/20 [39/1090] loss: 0.4303857237100601
train--5/20 [49/1090] loss: 0.45479041934013364
train--5/20 [59/1090] loss: 0.4582438081502914
train--5/20 [69/1090] loss: 0.44965647757053373
train--5/20 [79/1090] loss: 0.45427643060684203
train--5/20 [89/1090] loss: 0.4501740723848343
train--5/20 [99/1090] loss: 0.44237745702266695
train--5/20 [109/1090] loss: 0.4603251159191132
train--5/20 [119/1090] loss: 0.43930613100528715
train--5/20 [129/1090] loss: 0.4313389420509338
train--5/20 [139/1090] loss: 0.4456414431333542
train--5/20 [149/1090] loss: 0.468688428401947
train--5/20 [159/1090] loss: 0.44289965033531187
train--5/20 [169/1090] loss: 0.432914999127388
train--5/20 [179/1090] loss: 0.435601019859314
train--5/20 [189/1090] loss: 0.4386356443166733
train--5/20 [199/1090] loss: 0.43378276824951173
train--5/20 [209/1090] loss: 0.4461343139410019
train--5/20 [219/1090] loss: 0.4415082335472107
train--5/20 [229/1090] loss: 0.4278437614440918
train--5/20 [239/1090] loss: 0.42494758069515226
train--5/20 [249/1090] loss: 0.44238871335983276
train--5/20 [259/1090] loss: 0.44597986340522766
train--5/20 [269/1090] loss: 0.45238007605075836
train--5/20 [279/1090] loss: 0.4361850768327713
train--5/20 [289/1090] loss: 0.4607188910245895
train--5/20 [299/1090] loss: 0.46033255457878114
train--5/20 [309/1090] loss: 0.47396622598171234
train--5/20 [319/1090] loss: 0.4440009415149689
train--5/20 [329/1090] loss: 0.45666761994361876
train--5/20 [339/1090] loss: 0.46250449419021605
train--5/20 [349/1090] loss: 0.4565462678670883
train--5/20 [359/1090] loss: 0.4474083423614502
train--5/20 [369/1090] loss: 0.4519769847393036
train--5/20 [379/1090] loss: 0.4415678560733795
train--5/20 [389/1090] loss: 0.4525867223739624
train--5/20 [399/1090] loss: 0.4565749168395996
train--5/20 [409/1090] loss: 0.45211752951145173
train--5/20 [419/1090] loss: 0.4522087901830673
train--5/20 [429/1090] loss: 0.4525333195924759
train--5/20 [439/1090] loss: 0.4660208135843277
train--5/20 [449/1090] loss: 0.45663305222988126
train--5/20 [459/1090] loss: 0.4478154361248016
train--5/20 [469/1090] loss: 0.45518852174282076
train--5/20 [479/1090] loss: 0.45619526505470276
train--5/20 [489/1090] loss: 0.4489201307296753
train--5/20 [499/1090] loss: 0.45016891062259673
train--5/20 [509/1090] loss: 0.45611673295497895
train--5/20 [519/1090] loss: 0.4579495549201965
train--5/20 [529/1090] loss: 0.45064004361629484
train--5/20 [539/1090] loss: 0.43584770858287813
train--5/20 [549/1090] loss: 0.44924376308918
train--5/20 [559/1090] loss: 0.45424987077713014
train--5/20 [569/1090] loss: 0.43504922091960907
train--5/20 [579/1090] loss: 0.44450826942920685
train--5/20 [589/1090] loss: 0.4731201231479645
train--5/20 [599/1090] loss: 0.46118581891059873
train--5/20 [609/1090] loss: 0.46287866234779357
train--5/20 [619/1090] loss: 0.4497614026069641
train--5/20 [629/1090] loss: 0.4459529459476471
train--5/20 [639/1090] loss: 0.4427115708589554
train--5/20 [649/1090] loss: 0.465407007932663
train--5/20 [659/1090] loss: 0.44496587216854094
train--5/20 [669/1090] loss: 0.4524462342262268
train--5/20 [679/1090] loss: 0.47410960793495177
train--5/20 [689/1090] loss: 0.46591819524765016
train--5/20 [699/1090] loss: 0.4460872054100037
train--5/20 [709/1090] loss: 0.441843518614769
train--5/20 [719/1090] loss: 0.47315641641616824
train--5/20 [729/1090] loss: 0.46170111298561095
train--5/20 [739/1090] loss: 0.4448323220014572
train--5/20 [749/1090] loss: 0.4627071857452393
train--5/20 [759/1090] loss: 0.44568839967250823
train--5/20 [769/1090] loss: 0.455377134680748
train--5/20 [779/1090] loss: 0.4409162074327469
train--5/20 [789/1090] loss: 0.46840267479419706
train--5/20 [799/1090] loss: 0.47065354883670807
train--5/20 [809/1090] loss: 0.4546212315559387
